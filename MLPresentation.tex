\documentclass{beamer}
\usepackage[latin1]{inputenc}
\usepackage{listings}

\newcommand {\framedgraphic}[2] {
    \begin{frame}{#1}
        \begin{center}
            \includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{#2}
        \end{center}
    \end{frame}
}

\usetheme{Warsaw}
\title[Automatic differentiation and neural nets]{Automatic
  Differentiation\\Application to Machine Learning}
\author{Dominic Steinitz}
\institute{Kingston University}
\date{15 September 2013}
\begin{document}

\begin{frame}
\titlepage
\end{frame}

\section{Introduction}

\subsection{Preface}

\begin{frame}{What This Talk Is About}
Artificial Neural Networks are a well-established Machine Learning
technique. Traditionally, fitting is done using a technique called
backpropogation. This talk shows that this is just a specialised
version of {\em automatic differentiation} and explains this apparently
little known tool.
\end{frame}

\begin{frame}{The Goal}
\begin{itemize}
\item
Wish to fit a model using training data
\item
As an example assume model is a neural network
\item
Need to minimise a cost function: predicted vs. actual
\item
Highly non-linear
\item
Use steepest descent
\item
To do this we need the derivative of the cost function wrt the parameters
\end{itemize}
\end{frame}

\begin{frame}{But Problems \ldots}
\begin{itemize}
\item
Program calculates the non-linear function (not an explicit function)
\item
Function has lots of parameters
\end{itemize}
\end{frame}

\subsection{Bumping}

\begin{frame}{Bumping}
Could try bumping
$$
\frac{\partial E(\ldots, w, \ldots)}{\partial w} \approx \frac{E(\ldots, w + \epsilon, \ldots) - E(\ldots, w, \ldots)}{\epsilon}
$$
\begin{itemize}
\item
But we have many thousands of parameters
\item
And we are using floating point arithmetic \ldots
\end{itemize}
\end{frame}

\framedgraphic{Floating Point Errors}{diagrams/13a2bd186a0e123f040da9491fa98684.png}

\subsection{Symbolic Differentiation}
\begin{frame}[fragile]{Method}
\begin{itemize}
\item
Turn program into mathematical function
\item
Compute the differential also as a function either by hand or using
symbolic differentiation package.
\end{itemize}

But \ldots
\end{frame}

\begin{frame}[fragile]{Python Example}
\ldots consider the following Python fragment
\begin{lstlisting}[language=Python]
import numpy as np

def many_sines(x):
    y = x
    for i in range(1,7):
        y = np.sin(x+y)
    return y
\end{lstlisting}

When we unroll the loop we are actually evaluating

$$
f(x) = \sin(x + \sin(x + \sin(x + \sin(x + \sin(x + \sin(x + x))))))
$$

\end{frame}

\begin{frame}[fragile]{Blow Up}
Now suppose we want to get the differential of this
function. Symbolically this would be

$$
\begin{aligned}
f'(x) &=           (((((2\cdot \cos(2x)+1)\cdot \\
      &\phantom{=} \cos(\sin(2x)+x)+1)\cdot \\
      &\phantom{=} \cos(\sin(\sin(2x)+x)+x)+1)\cdot \\
      &\phantom{=} \cos(\sin(\sin(\sin(2x)+x)+x)+x)+1)\cdot \\
      &\phantom{=} \cos(\sin(\sin(\sin(\sin(2x)+x)+x)+x)+x)+1)\cdot \\
      &\phantom{=} \cos(\sin(\sin(\sin(\sin(\sin(2x)+x)+x)+x)+x)+x)
\end{aligned}
$$
\end{frame}

\begin{frame}[fragile]{Implications for ANN}
\begin{itemize}
\item
The mathematical function for an ANN could easily be more
complicated.
\item
Creating the derivative by hand is error prone.
\item
To use a symbolic differentiation requires some manipulation of the
original program.
\end{itemize}
\end{frame}

\section{Chain Rule Refresher}

\begin{frame}[fragile]{The Rule}
$$
(g \circ f)'(a) = g'(f(a))\cdot f'(a)
$$

in alternative notation

$$
\frac{\mathrm{d} (g \circ f)}{\mathrm{d} x}(a) =
\frac{\mathrm{d} g}{\mathrm{d} y}(f(a)) \frac{\mathrm{d} f}{\mathrm{d} x}(a)
$$

where $y = f(x)$. More suggestively, setting $h = g \circ f$, we can write

$$
\frac{\mathrm{d} h}{\mathrm{d} x} =
\frac{\mathrm{d} h}{\mathrm{d} y} \frac{\mathrm{d} y}{\mathrm{d} x}
$$

where it is understood that $\mathrm{d} h / \mathrm{d} x$ and
$\mathrm{d} y / \mathrm{d} x$ are evaluated at $a$ and $\mathrm{d} h /
\mathrm{d} y$ is evaluated at $f(a)$.
\end{frame}

\begin{frame}[fragile]{Examples}
Recall that if $f(x) = x^2$ then $f'(x) = 2x$. Suppose we only know
this and we also know the chain rule then we can use this to calculate
the derivative of $f(x) = x^4$.

We have
\begin{itemize}
\item
$f(x) = x^2$ and $g(y) = y^2$ thus $(g \circ f)(x) = x^4$
\item
$f'(x) = 2x$ and $g'(y) = 2y$
\item
The Chain Rule $(g \circ f)'(x) = g'(f(x))f'(x)$
\item
Thus $(g \circ f)'(x) = 2f(x)2x = 2x^22x = 4x^3$
\end{itemize}
\end{frame}

\section{Neural Network Refresher}

\begin{frame}[fragile]{ANN: The Classical Approach}
Rather than using bumping or symbolic differentation, for ANN's we can
use backpropagation, a technique which seems to be taught on many
Machine Learning courses.
\end{frame}

\begin{frame}[fragile]{The Model}
Here is our model, with $\boldsymbol{x}$ the input,
$\hat{\boldsymbol{y}}$ the predicted output and $\boldsymbol{y}$ the
actual output and $w^{(k)}$ the weights in the $k$-th layer. We have
concretised the transfer function as $\tanh$ but it is quite popular
to use the $\text{logit}$ function.
\end{frame}

\framedgraphic{Example Neural Net}{diagrams/ca75393cd25ce951edcd7133da24a2c6.png}

\begin{frame}[fragile]{The Model}
$$
\begin{aligned}
a_i^{(1)}   &= \sum_{j=0}^{N^{(1)}} w_{ij}^{(1)} x_j       & z_i^{(1)} &= \tanh(a_i^{(1)}) \\
a_i^{(2)}   &= \sum_{j=0}^{N^{(2)}} w_{ij}^{(2)} z_j^{(1)} &  \dots     &= \ldots \\
a_i^{(L-1)} &= \sum_{j=0}^{N^{(L-1)}} w_{ij}^{(L-1)} z_j^{(L-2)} & z_j^{(L-1)} &= \tanh(a_j^{(L-1)}) \\
\hat{y}_i  &= \sum_{j=0}^{N^{(L)}} w_{ij}^{(L)} z_j^{(L-1)} \\
\end{aligned}
$$
\end{frame}

\begin{frame}[fragile]{The Model}
Loss function aka cost function

$$
E(\boldsymbol{w}; \boldsymbol{x}, \boldsymbol{y}) = \frac{1}{2}\|(\hat{\boldsymbol{y}} - \boldsymbol{y})\|^2
$$

To fit the model we need to find the weights which minimise the cost
function.
\end{frame}

\begin{frame}[fragile]{Steepest Descent}
For each weight, we find the derivative of the cost function wrt that
weight and use that to step a small distance (the learning rate) in
the direction which reduces the cost most quickly.

In summary:

$$
w' = w - \gamma\nabla E(w)
$$
\end{frame}

\begin{frame}[fragile]{The Model}
In order to perform steepest descent, we need to calculate

$$
\Delta w_{ij} \equiv \frac{\partial E}{\partial w_{ij}}
$$

Applying the chain rule

$$
\Delta w_{ij} =
\frac{\partial E}{\partial w_{ij}} =
\frac{\partial E}{\partial a_i}\frac{\partial a_i}{\partial w_{ij}}
$$
\end{frame}

\begin{frame}[fragile]{The Model}
Since

$$
a_j^{(l)} = \sum_{i=0}^N w_{ij}^{(l)}z_i^{(l-1)}
$$

we have

$$
\frac{\partial a_i^{(l)}}{\partial w_{ij}^{(l)}} =
\frac{\partial}{\partial w_{ij}^{(l)}}\sum_{k=0}^M w_{kj}^{(l)}z_k^{(l-1)} =
z_i^{(l-1)}
$$
\end{frame}

\begin{frame}[fragile]{The Model}
Defining

$$
\delta_j^{(l)} \equiv
\frac{\partial E}{\partial a_j^{(l)}}
$$

we obtain

$$
\Delta w_{ij}^{(l)} =
\frac{\partial E}{\partial w_{ij}^{(l)}} =
\delta_j^{(l)} z_i^{(l-1)}
$$
\end{frame}

\begin{frame}[fragile]{The Model}
Finding the $z_i$ for each layer is straightforward: we start with the
inputs and propagate forward. In order to find the $\delta_j$ we need
to start with the outputs a propagate backwards:

For the output layer we have (since $\hat{y}_j = a_j$)
$$
\delta_j = \frac{\partial E}{\partial a_j} = \frac{\partial E}{\partial y_j} = \frac{\partial}{\partial y_j}\bigg(\frac{1}{2}\sum_{i=0}^M (\hat{y}_i - y_i)^2\bigg) = \hat{y}_j - y_j
$$
\end{frame}

\begin{frame}[fragile]{The Model}
For a hidden layer using the chain rule

$$
\delta_j^{(l-1)} = \frac{\partial E}{\partial a_j^{(l-1)}} =
\sum_k \frac{\partial E}{\partial a_k^{(l)}}\frac{\partial a_k^{(l)}}{\partial a_j^{(l-1)}}
$$
\end{frame}

\begin{frame}[fragile]{The Model}
First consider

$$
\frac{\partial a_k^{(l)}}{\partial a_j^{(l-1)}}
$$

We have

$$
a_k^{(l)} = \sum_i w_{ki}^{(l)}z_i^{(l-1)} = \sum_i w_{ki}^{(l)} f(a_i^{(l-1)})
$$

so that

$$
\frac{\partial a_k^{(l)}}{\partial a_j^{(l-1)}} =
\frac{\sum_i w_{ki}^{(l)} f(a_i^{(l-1)})}{\partial a_j^{(l-1)}} =
w_{kj}^{(l)}\,f'(a_j^{(l-1)})
$$
\end{frame}

\begin{frame}[fragile]{The Model}
and thus

$$
\delta_j^{(l-1)} =
\sum_k \frac{\partial E}{\partial a_k^{(l)}}\frac{\partial a_k^{(l)}}{\partial a_j^{(l-1)}} =
\sum_k \delta_k^{(l)} w_{kj}^{(l)}\, f'(a_j^{(l-1)}) =
f'(a_j^{(l-1)}) \sum_k \delta_k^{(l)} w_{kj}^{(l)}
$$
\end{frame}

\begin{frame}[fragile]{The Model}
Summarising

\begin{enumerate}
\item
We calculate all $a_j$ and $z_j$ for each layer starting with the
input layer and propagating forward.
\item
We evaluate $\delta_j^{(L)}$ in the output layer using $\delta_j = \hat{y}_j - y_j$.
\item
We evaluate $\delta_j$ in each layer using $\delta_j^{(l-1)} =
f'(a_j^{(l-1)})\sum_k \delta_k^{(l)} w_{kj}^{(l)}$ starting with the output
layer and propagating backwards.
\item
Use $\partial E / \partial w_{ij}^{(l)} = \delta_j^{(l)} z_i^{(l-1)}$ to obtain the
required derivatives in each layer.
\end{enumerate}
\end{frame}

\section{Generalising}

\begin{frame}[fragile]{The Model}
\begin{itemize}
\item
Phew!
\item
But maybe following P\'{o}lya we can find a problem that is more
general and use this not just for ANNs but for estimating parameters
for other Machine Learning models.
\item
Idea: we drew the Neural Net as a graph maybe if we can draw a program as a
graph we can calculate the derivative of the function which the
program represents.
\end{itemize}
\end{frame}

\subsection{Reverse Mode Automatic Differentiation}

\begin{frame}[fragile]{The Model}
Consider the function

$$
f(x) = \exp(\exp(x) + (\exp(x))^2) + \sin(\exp(x) + (\exp(x))^2)
$$

We can write this as a data flow graph...
\end{frame}

\framedgraphic{Test}{diagrams/02c0671aa558b88e5ed6f195b22bbd8a.png}

\begin{frame}[fragile]{Sequence of Functions}
We can thus re-write our function as a sequence of simpler functions
in which each function only depends on variables earlier in the
sequence.

$$
\begin{aligned}
u_7    &= f_7(u_6, u_5, u_4, u_3, u_2, u_1) \\
u_6    &= f_6(u_5, u_4, u_3, u_2, u_1) \\
\ldots &= \ldots \\
u_2    &= f_2(u_1)
\end{aligned}
$$
\end{frame}

\begin{frame}[fragile]{Differentials}
$$
\begin{aligned}
\mathrm{d}u_7    &= \frac{\partial f_7}{\partial u_6} \mathrm{d} u_6 +
                    \frac{\partial f_7}{\partial u_5} \mathrm{d} u_5 +
                    \frac{\partial f_7}{\partial u_4} \mathrm{d} u_4 +
                    \frac{\partial f_7}{\partial u_3} \mathrm{d} u_3 +
                    \frac{\partial f_7}{\partial u_2} \mathrm{d} u_2 +
                    \frac{\partial f_7}{\partial u_1} \mathrm{d} u_1 \\
\mathrm{d}u_6    &= \frac{\partial f_6}{\partial u_5} \mathrm{d} u_5 +
                    \frac{\partial f_6}{\partial u_4} \mathrm{d} u_4 +
                    \frac{\partial f_6}{\partial u_3} \mathrm{d} u_3 +
                    \frac{\partial f_6}{\partial u_2} \mathrm{d} u_2 +
                    \frac{\partial f_6}{\partial u_1} \mathrm{d} u_1 \\
\ldots           &= \ldots \\
\mathrm{d}u_2    &= \frac{\partial f_2}{\partial u_1} \mathrm{d} u_1
\end{aligned}
$$
\end{frame}

\begin{frame}[fragile]{Differentials Example}
In our particular example, since $u_1, \dots, u_5$ do not depend on $u_6$

$$
\begin{aligned}
\frac{\mathrm{d}u_7}{\mathrm{d}u_6} &= 1
\end{aligned}
$$

Further $u_6$ does not depend on $u_5$ so we also have

$$
\begin{aligned}
\frac{\mathrm{d}u_7}{\mathrm{d}u_5} &= 1 \\
\end{aligned}
$$
\end{frame}

\begin{frame}[fragile]{Differentials Example More Interesting}
Now things become more interesting as $u_6$ and $u_5$ both depend on
$u_4$ and so the chain rule makes an explicit appearance

$$
\begin{aligned}
\frac{\mathrm{d}u_7}{\mathrm{d}u_4} &=
 \frac{\mathrm{d}u_7}{\mathrm{d}u_6}\frac{\mathrm{d}u_6}{\mathrm{d}u_4} +
 \frac{\mathrm{d}u_7}{\mathrm{d}u_5}\frac{\mathrm{d}u_5}{\mathrm{d}u_4} \\
&= \frac{\mathrm{d}u_7}{\mathrm{d}u_6}\exp{u_4} +
   \frac{\mathrm{d}u_7}{\mathrm{d}u_5}\cos{u_5}
\end{aligned}
$$
\end{frame}

\begin{frame}[fragile]{Differentials Example More Interesting}
Carrying on

$$
\begin{aligned}
\frac{\mathrm{d}u_7}{\mathrm{d}u_3} &=
 \frac{\mathrm{d}u_7}{\mathrm{d}u_4}\frac{\mathrm{d}u_4}{\mathrm{d}u_3} \\
&= \frac{\mathrm{d}u_7}{\mathrm{d}u_4} \\
\frac{\mathrm{d}u_7}{\mathrm{d}u_2} &=
 \frac{\mathrm{d}u_7}{\mathrm{d}u_4}\frac{\mathrm{d}u_4}{\mathrm{d}u_2} +
 \frac{\mathrm{d}u_7}{\mathrm{d}u_3}\frac{\mathrm{d}u_3}{\mathrm{d}u_2} \\
&= \frac{\mathrm{d}u_7}{\mathrm{d}u_4} + 2u_2\frac{\mathrm{d}u_7}{\mathrm{d}u_4} \\
\frac{\mathrm{d}u_7}{\mathrm{d}u_1} &=
 \frac{\mathrm{d}u_7}{\mathrm{d}u_2}\frac{\mathrm{d}u_2}{\mathrm{d}u_1} \\
&=\frac{\mathrm{d}u_7}{\mathrm{d}u_2}\exp{u_2}
\end{aligned}
$$
\end{frame}

\begin{frame}[fragile]{Reverse Mode Conclusions}
\begin{itemize}
\item
Note that having worked from top to bottom (the forward sweep) in the
graph to calculate the function itself, we have to work backwards from
bottom to top (the backward sweep) to calculate the derivative.
\item
So provided we can translate our program into a call graph, we can
apply this procedure to calculate the differential with the same
complexity as the original program.
\item
The pictorial representation of an ANN is effectively the data flow
graph of the cost function (without the final cost calculation itself)
and its differential can be calculated as just being identical to
backpropagation.
\end{itemize}
\end{frame}

\subsection{Forward Mode Automatic Differentiation}

\begin{frame}[fragile]{Dual Numbers}
\begin{itemize}
\item
An alternative method for automatic differentiation is called forward
mode and has a simple implementation. Let us illustrate this using
[Haskell 98]. The actual implementation is about 20 lines of code.
\item
Let us define dual numbers

\begin{lstlisting}[language=Haskell]
data Dual = Dual Double Double
  deriving (Eq, Show)
\end{lstlisting}
\item
We can think of these pairs as first order polynomials in the
indeterminate $\epsilon$, $x + \epsilon x'$ such that $\epsilon^2 = 0$
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Dual Numbers}
Thus, for example, we have

$$
\begin{aligned}
(x + \epsilon x') + (y + \epsilon y') &= ((x + y) + \epsilon (x' + y')) \\
(x + \epsilon x')(y + \epsilon y') &= xy + \epsilon (xy' + x'y) \\
\log (x + \epsilon x') &=
\log x (1 + \epsilon \frac {x'}{x}) =
\log x + \epsilon\frac{x'}{x} \\
\sqrt{(x + \epsilon x')} &=
\sqrt{x(1 + \epsilon\frac{x'}{x})} =
\sqrt{x}(1 + \epsilon\frac{1}{2}\frac{x'}{x}) =
\sqrt{x} + \epsilon\frac{1}{2}\frac{x'}{\sqrt{x}} \\
\ldots &= \ldots
\end{aligned}
$$
\end{frame}

\begin{frame}[fragile]{Dual Numbers and The Chain Rule}
Notice that these equations implicitly encode the chain rule. For
example, we know, using the chain rule, that

$$
\frac{\mathrm{d}}{\mathrm{d} x}\log(\sqrt x) =
\frac{1}{\sqrt x}\frac{1}{2}x^{-1/2} =
\frac{1}{2x}
$$

And using the example equations above we have

$$
\begin{aligned}
\log(\sqrt {x + \epsilon x'}) &= \log (\sqrt{x} + \epsilon\frac{1}{2}\frac{x'}{\sqrt{x}}) \\
                              &= \log (\sqrt{x}) + \epsilon\frac{\frac{1}{2}\frac{x'}{\sqrt{x}}}{\sqrt{x}} \\
                              &= \log (\sqrt{x}) + \epsilon x'\frac{1}{2x}
\end{aligned}
$$
\end{frame}

\begin{frame}[fragile]{Dual Numbers Example}
Notice that dual numbers carry around the calculation and the
derivative of the calculation.

\begin{itemize}
\item
To actually evaluate $\log(\sqrt{x})$
at a particular value, say 2, we plug in 2 for $x$ and 1 for $x'$
$$
\log (\sqrt{2 + \epsilon 1}) = \log(\sqrt{2}) + \epsilon\frac{1}{4}
$$
\item
Thus the derivative of $\log(\sqrt{x})$ at 2 is $1/4$.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Dual Numbers are Numbers}
\begin{scriptsize}
\begin{lstlisting}[language=Haskell]
instance Num Dual where
  fromInteger n             = constD (fromInteger n)
  (Dual x x') + (Dual y y') = Dual (x + y) (x' + y')
  (Dual x x') * (Dual y y') = Dual (x * y) (x * y' + y * x')
  negate (Dual x x')        = Dual (negate x) (negate x')
  signum _                  = undefined
  abs _                     = undefined
\end{lstlisting}
\end{scriptsize}
\end{frame}

\end{document}
