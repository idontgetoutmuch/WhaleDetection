\documentclass{beamer}
\usepackage[latin1]{inputenc}
\usepackage{listings}

\newcommand {\framedgraphic}[2] {
    \begin{frame}{#1}
        \begin{center}
            \includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{#2}
        \end{center}
    \end{frame}
}

\usetheme{Warsaw}
\title[Automatic differentiation and neural nets]{Automatic
  Differentiation\\Application to Machine Learning}
\author{Dominic Steinitz}
\institute{Kingston University}
\date{15 September 2013}
\begin{document}

\begin{frame}
\titlepage
\end{frame}

\section{Introduction}

\subsection{Preface}

\begin{frame}{What This Talk Is About}
Artificial Neural Networks are a well-established Machine Learning
technique. Traditionally, fitting is done using a technique called
backpropogation. This talk shows that this is just a specialised
version of {\em automatic differentiation} and explains this apparently
little known tool.
\end{frame}

\begin{frame}{The Goal}
\begin{itemize}
\item
Wish to fit a model using training data
\item
As an example assume model is a neural network
\item
Need to minimise a cost function: predicted vs. actual
\item
Highly non-linear
\item
Use steepest descent
\item
To do this we need the derivative of the cost function wrt the parameters
\end{itemize}
\end{frame}

\begin{frame}{But Problems \ldots}
\begin{itemize}
\item
Program calculates the non-linear function (not an explicit function)
\item
Function has lots of parameters
\end{itemize}
\end{frame}

\subsection{Bumping}

\begin{frame}{Bumping}
Could try bumping
$$
\frac{\partial E(\ldots, w, \ldots)}{\partial w} \approx \frac{E(\ldots, w + \epsilon, \ldots) - E(\ldots, w, \ldots)}{\epsilon}
$$
\begin{itemize}
\item
But we have many thousands of parameters
\item
And we are using floating point arithmetic \ldots
\end{itemize}
\end{frame}

\framedgraphic{Floating Point Errors}{diagrams/13a2bd186a0e123f040da9491fa98684.png}

\subsection{Symbolic Differentiation}
\begin{frame}[fragile]{Method}
\begin{itemize}
\item
Turn program into mathematical function
\item
Compute the differential also as a function either by hand or using
symbolic differentiation package.
\end{itemize}

But \ldots
\end{frame}

\begin{frame}[fragile]{Python Example}
\ldots consider the following Python fragment
\begin{lstlisting}[language=Python]
import numpy as np

def many_sines(x):
    y = x
    for i in range(1,7):
        y = np.sin(x+y)
    return y
\end{lstlisting}

When we unroll the loop we are actually evaluating

$$
f(x) = \sin(x + \sin(x + \sin(x + \sin(x + \sin(x + \sin(x + x))))))
$$

\end{frame}

\begin{frame}[fragile]{Blow Up}
Now suppose we want to get the differential of this
function. Symbolically this would be

$$
\begin{aligned}
f'(x) &=           (((((2\cdot \cos(2x)+1)\cdot \\
      &\phantom{=} \cos(\sin(2x)+x)+1)\cdot \\
      &\phantom{=} \cos(\sin(\sin(2x)+x)+x)+1)\cdot \\
      &\phantom{=} \cos(\sin(\sin(\sin(2x)+x)+x)+x)+1)\cdot \\
      &\phantom{=} \cos(\sin(\sin(\sin(\sin(2x)+x)+x)+x)+x)+1)\cdot \\
      &\phantom{=} \cos(\sin(\sin(\sin(\sin(\sin(2x)+x)+x)+x)+x)+x)
\end{aligned}
$$
\end{frame}

\begin{frame}[fragile]{Implications for ANN}
\begin{itemize}
\item
The mathematical function for an ANN could easily be more
complicated.
\item
Creating the derivative by hand is error prone.
\item
To use a symbolic differentiation requires some manipulation of the
original program.
\end{itemize}
\end{frame}

\section{Chain Rule Refresher}

\begin{frame}[fragile]{The Rule}
$$
(g \circ f)'(a) = g'(f(a))\cdot f'(a)
$$

in alternative notation

$$
\frac{\mathrm{d} (g \circ f)}{\mathrm{d} x}(a) =
\frac{\mathrm{d} g}{\mathrm{d} y}(f(a)) \frac{\mathrm{d} f}{\mathrm{d} x}(a)
$$

where $y = f(x)$. More suggestively, setting $h = g \circ f$, we can write

$$
\frac{\mathrm{d} h}{\mathrm{d} x} =
\frac{\mathrm{d} h}{\mathrm{d} y} \frac{\mathrm{d} y}{\mathrm{d} x}
$$

where it is understood that $\mathrm{d} h / \mathrm{d} x$ and
$\mathrm{d} y / \mathrm{d} x$ are evaluated at $a$ and $\mathrm{d} h /
\mathrm{d} y$ is evaluated at $f(a)$.
\end{frame}

\begin{frame}[fragile]{Examples}
Recall that if $f(x) = x^2$ then $f'(x) = 2x$. Suppose we only know
this and we also know the chain rule then we can use this to calculate
the derivative of $f(x) = x^4$.

We have
\begin{itemize}
\item
$f(x) = x^2$ and $g(y) = y^2$ thus $(g \circ f)(x) = x^4$
\item
$f'(x) = 2x$ and $g'(y) = 2y$
\item
The Chain Rule $(g \circ f)'(x) = g'(f(x))f'(x)$
\item
Thus $(g \circ f)'(x) = 2f(x)2x = 2x^22x = 4x^3$
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Examples}
$$
\frac{\mathrm{d}}{\mathrm{d} x} \sqrt{3 \sin(x)} =
\frac{\mathrm{d}}{\mathrm{d} x} (3 \sin(x)) \cdot \frac{\mathrm{d}}{\mathrm{d} y} \sqrt{y} =
3 \cos(x) \cdot \frac{1}{2\sqrt{y}} =
\frac{3\cos(x)}{2\sqrt{3\sin(x)}}
$$
\end{frame}

\section{Neural Network Refresher}

\begin{frame}[fragile]{ANN: The Classical Approach}
Rather than using bumping or symbolic differentation, for ANN's we can
use backpropagation, a technique which seems to be taught on many
Machine Learning courses.
\end{frame}

\begin{frame}[fragile]{The Model}
Here is our model, with $\boldsymbol{x}$ the input,
$\hat{\boldsymbol{y}}$ the predicted output and $\boldsymbol{y}$ the
actual output and $w^{(k)}$ the weights in the $k$-th layer. We have
concretised the transfer function as $\tanh$ but it is quite popular
to use the $\text{logit}$ function.
\end{frame}

\framedgraphic{Example Neural Net}{diagrams/ca75393cd25ce951edcd7133da24a2c6.png}

\begin{frame}[fragile]{The Model}
$$
\begin{aligned}
a_i^{(1)}   &= \sum_{j=0}^{N^{(1)}} w_{ij}^{(1)} x_j       & z_i^{(1)} &= \tanh(a_i^{(1)}) \\
a_i^{(2)}   &= \sum_{j=0}^{N^{(2)}} w_{ij}^{(2)} z_j^{(1)} &  \dots     &= \ldots \\
a_i^{(L-1)} &= \sum_{j=0}^{N^{(L-1)}} w_{ij}^{(L-1)} z_j^{(L-2)} & z_j^{(L-1)} &= \tanh(a_j^{(L-1)}) \\
\hat{y}_i  &= \sum_{j=0}^{N^{(L)}} w_{ij}^{(L)} z_j^{(L-1)} \\
\end{aligned}
$$
\end{frame}

\begin{frame}[fragile]{The Model}
Loss function aka cost function

$$
E(\boldsymbol{w}; \boldsymbol{x}, \boldsymbol{y}) = \frac{1}{2}\|(\hat{\boldsymbol{y}} - \boldsymbol{y})\|^2
$$

To fit the model we need to find the weights which minimise the cost
function.
\end{frame}

\begin{frame}[fragile]{Steepest Descent}
For each weight, we find the derivative of the cost function wrt that
weight and use that to step a small distance (the learning rate) in
the direction which reduces the cost most quickly.

In summary:

$$
w' = w - \gamma\nabla E(w)
$$
\end{frame}

\begin{frame}[fragile]{The Model}
In order to perform steepest descent, we need to calculate

$$
\Delta w_{ij} \equiv \frac{\partial E}{\partial w_{ij}}
$$

Applying the chain rule

$$
\Delta w_{ij} =
\frac{\partial E}{\partial w_{ij}} =
\frac{\partial E}{\partial a_i}\frac{\partial a_i}{\partial w_{ij}}
$$
\end{frame}

\begin{frame}[fragile]{The Model}
Since

$$
a_j^{(l)} = \sum_{i=0}^N w_{ij}^{(l)}z_i^{(l-1)}
$$

we have

$$
\frac{\partial a_i^{(l)}}{\partial w_{ij}^{(l)}} =
\frac{\partial}{\partial w_{ij}^{(l)}}\sum_{k=0}^M w_{kj}^{(l)}z_k^{(l-1)} =
z_i^{(l-1)}
$$
\end{frame}

\begin{frame}[fragile]{The Model}
Defining

$$
\delta_j^{(l)} \equiv
\frac{\partial E}{\partial a_j^{(l)}}
$$

we obtain

$$
\Delta w_{ij}^{(l)} =
\frac{\partial E}{\partial w_{ij}^{(l)}} =
\delta_j^{(l)} z_i^{(l-1)}
$$
\end{frame}

\begin{frame}[fragile]{The Model}
Finding the $z_i$ for each layer is straightforward: we start with the
inputs and propagate forward. In order to find the $\delta_j$ we need
to start with the outputs a propagate backwards:

For the output layer we have (since $\hat{y}_j = a_j$)
$$
\delta_j = \frac{\partial E}{\partial a_j} = \frac{\partial E}{\partial y_j} = \frac{\partial}{\partial y_j}\bigg(\frac{1}{2}\sum_{i=0}^M (\hat{y}_i - y_i)^2\bigg) = \hat{y}_j - y_j
$$
\end{frame}

\begin{frame}[fragile]{The Model}
For a hidden layer using the chain rule

$$
\delta_j^{(l-1)} = \frac{\partial E}{\partial a_j^{(l-1)}} =
\sum_k \frac{\partial E}{\partial a_k^{(l)}}\frac{\partial a_k^{(l)}}{\partial a_j^{(l-1)}}
$$
\end{frame}

\section{Other}

\framedgraphic{Test}{diagrams/02c0671aa558b88e5ed6f195b22bbd8a.png}

\end{document}
