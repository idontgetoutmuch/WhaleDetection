\documentclass{beamer}
\usepackage[latin1]{inputenc}
\usepackage{listings}
\usepackage{color}

\newcommand {\framedgraphic}[2] {
    \begin{frame}{#1}
        \begin{center}
            \includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{#2}
        \end{center}
    \end{frame}
}

\usetheme{Warsaw}
\title[Automatic differentiation]{Automatic Differentiation\\A Criminally Underused Tool?}
\author{Dominic Steinitz}
\institute{Kingston University}
\date{15 September 2013}
\begin{document}

\begin{frame}
\titlepage
\end{frame}

\section*{Outline}
    \frame{\tableofcontents}

\section{Introduction}

\subsection{What This Talk is About}

\begin{frame}{What This Talk Is About}

  \begin{itemize}
  \item Artificial Neural Networks (ANNs) are a well-established
    Machine Learning (ML) technique.
  \item Traditionally, training ANNs is done using a technique called
    backpropogation.
  \item It turns out that this is just a specialised version of
    Reverse Automatic Differentiation (RAD) and steepest descent.
  \item This talk explains the theory behind RAD and develops an
    implementation in Haskell of Forward Automatic Differentiation
    (FAD) with a brief application to another ML technique:
    regression.
  \end{itemize}

\end{frame}

\section{A Neural Network Refresher}

\subsection{A Problem}

\framedgraphic{Problem: Recognise Hand Written Digits}{diagrams/mnist_originals.png}

\begin{frame}[fragile]{A Simpler Problem}
\begin{itemize}
\item We wish to recognize the digits 0 and 1.
\item Many pixels per observation
\item Many observations
\end{itemize}
\end{frame}

\framedgraphic{The Data for 0s}{diagrams/mnist_train0.jpg}

\framedgraphic{The Data for 1s}{diagrams/mnist_train1.jpg}

\subsection{A Solution}

\framedgraphic{Perceptron}{diagrams/Fita1.png}

\begin{frame}[fragile]{In Symbols}
$$
a = \sum_{j=0}^{N} w_{j} x_j
$$

$$
\hat{y} = f(a)
$$

To fit the model we need to find the weights which minimise the cost
(aka loss) function.

$$
E(\boldsymbol{w}; \boldsymbol{x}, \boldsymbol{y}) =
\frac{1}{2}\sum_{i=1}^M(\hat{\boldsymbol{y}_i} - \boldsymbol{y}_i)^2
$$
\end{frame}

\begin{frame}[fragile]{Steepest Descent}
\uncover<1->{
For each weight, we find the derivative of the cost function wrt that
weight and use that to step a small distance (the learning rate) in
the direction which reduces the cost most quickly.
}

\uncover<2->{
In symbols:
$$
w' = w - \gamma\nabla E(w)
$$

where $\gamma$ is the step length aka the learning rate.
}
\end{frame}

\begin{frame}[fragile]{In Summary $\ldots$}
\begin{itemize}
\item Hand calculate the derivative
\item Code it up
\item Use steepest descent with hand calculated derivative
\end{itemize}
\end{frame}

\framedgraphic{Multi Layer Perceptron (MLP)}{diagrams/Fita2.png}

\begin{frame}[fragile]{The Derivative Problem}
\begin{itemize}
\item Hand calculating the derivative is more complicated and thus error prone.
\item What happens if I change my model?
\item What happens if I change my transfer function?
\end{itemize}
\end{frame}

\section{Differentiation Methods}

\subsection{Bumping}

\begin{frame}{Bumping}
Could try bumping
$$
\frac{\partial E(\ldots, w, \ldots)}{\partial w} \approx \frac{E(\ldots, w + \epsilon, \ldots) - E(\ldots, w, \ldots)}{\epsilon}
$$
\begin{itemize}
\item
But we have many thousands of parameters
\item
And we are using floating point arithmetic \ldots
\end{itemize}
\end{frame}

\framedgraphic{Floating Point Errors}{diagrams/13a2bd186a0e123f040da9491fa98684.png}

\subsection{Symbolic Differentiation}

\begin{frame}[fragile]{Python Example}
  Could use a symbolic differentiation package but consider the
  following Python fragment
\begin{lstlisting}[language=Python]
import numpy as np

def many_sines(x):
    y = x
    for i in range(1,7):
        y = np.sin(x+y)
    return y
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Python Example}
When we unroll the loop we are actually evaluating

$$
f(x) = \sin(x + \sin(x + \sin(x + \sin(x + \sin(x + \sin(x + x))))))
$$

\end{frame}

\begin{frame}[fragile]{Blow Up}
Now suppose we want to get the differential of this
function. Symbolically this would be

$$
\begin{aligned}
f'(x) &=           (((((2\cdot \cos(2x)+1)\cdot \\
      &\phantom{=} \cos(\sin(2x)+x)+1)\cdot \\
      &\phantom{=} \cos(\sin(\sin(2x)+x)+x)+1)\cdot \\
      &\phantom{=} \cos(\sin(\sin(\sin(2x)+x)+x)+x)+1)\cdot \\
      &\phantom{=} \cos(\sin(\sin(\sin(\sin(2x)+x)+x)+x)+x)+1)\cdot \\
      &\phantom{=} \cos(\sin(\sin(\sin(\sin(\sin(2x)+x)+x)+x)+x)+x)
\end{aligned}
$$
\end{frame}

\begin{frame}[fragile]{Moreover}
\begin{itemize}
\item The mathematical function for an MLP could easily be more
  complicated.
\item To use a symbolic differentiation package (usually) requires
  some manipulation of the original program.
\end{itemize}
\end{frame}

\subsection{Backpropagation}

\begin{frame}[fragile]{Backpropagation}
\begin{itemize}
% \pause
\item Efficient method for finding the derivative for MLPs (1960's).
% \pause
\item Complex but quite often appears in Machine Learning 101.
% \pause
\item But maybe following P\'{o}lya we can find a problem that is more
  general and use this not just for MLPs but for estimating parameters
  for other Machine Learning models.
% \pause
\item Idea: we drew the Neural Net as a graph maybe if we can draw a
  program as a graph we can calculate the derivative of the function
  which the program represents.
\end{itemize}
\end{frame}

\section{Automatic Differentiation}

\subsection{Reverse Mode}

\begin{frame}[fragile]{A Simpler Example}
Instead of an MLP, consider the function

$$
f(x) = \exp(\exp(x) + (\exp(x))^2) + \sin(\exp(x) + (\exp(x))^2)
$$

We can write this as a data flow graph...
\end{frame}

\framedgraphic{Data Flow Graph}{diagrams/02c0671aa558b88e5ed6f195b22bbd8a.png}

\begin{frame}[fragile]{Sequence of Functions}
We can thus re-write our function as a sequence of simpler functions
in which each function only depends on variables earlier in the
sequence.

$$
\begin{aligned}
u_7    &= f_7(u_6, u_5, u_4, u_3, u_2, u_1) \\
u_6    &= f_6(u_5, u_4, u_3, u_2, u_1) \\
\ldots &= \ldots \\
u_2    &= f_2(u_1)
\end{aligned}
$$
\end{frame}

\begin{frame}[fragile]{Differentials}
$$
\begin{aligned}
\mathrm{d}u_7    &= \frac{\partial f_7}{\partial u_6} \mathrm{d} u_6 +
                    \frac{\partial f_7}{\partial u_5} \mathrm{d} u_5 +
                    \frac{\partial f_7}{\partial u_4} \mathrm{d} u_4 +
                    \frac{\partial f_7}{\partial u_3} \mathrm{d} u_3 +
                    \frac{\partial f_7}{\partial u_2} \mathrm{d} u_2 +
                    \frac{\partial f_7}{\partial u_1} \mathrm{d} u_1 \\
\mathrm{d}u_6    &= \frac{\partial f_6}{\partial u_5} \mathrm{d} u_5 +
                    \frac{\partial f_6}{\partial u_4} \mathrm{d} u_4 +
                    \frac{\partial f_6}{\partial u_3} \mathrm{d} u_3 +
                    \frac{\partial f_6}{\partial u_2} \mathrm{d} u_2 +
                    \frac{\partial f_6}{\partial u_1} \mathrm{d} u_1 \\
\ldots           &= \ldots \\
\mathrm{d}u_2    &= \frac{\partial f_2}{\partial u_1} \mathrm{d} u_1
\end{aligned}
$$
\end{frame}

\begin{frame}[fragile]{Tangents and Cotangents}
  If you are worried by differentials, this is all well defined. For
  example, suppose we have $f : \mathbb{R}^2 \rightarrow \mathbb{R}$
  then derivative of $f$ at $a$, denoted $\mathrm{d}f_a$, is an
  element of a vector space called the cotangent space and

$$
\mathrm{d}f_a = \bigg(\frac{\partial f}{\partial x}\bigg)_a\mathrm{d}x_a +
                \bigg(\frac{\partial f}{\partial y}\bigg)_a\mathrm{d}y_a
$$

where $\mathrm{d}x_a, \mathrm{d}y_a$ are a basis for the cotangent
space and $\big(\frac{\partial f}{\partial x}\big)_a,
\big(\frac{\partial f}{\partial x}\big)_a$ are real numbers.
\end{frame}

\begin{frame}[fragile]{Tangents and Cotangents}
All we have to know is the following form of the {\color{blue}{chain rule}}.

$$
\frac{\partial f}{\partial u} =
 \frac{\partial f}{\partial x}\frac{\partial x}{\partial u} +
 \frac{\partial f}{\partial y}\frac{\partial y}{\partial u}
$$

\end{frame}

\begin{frame}[fragile]{Differentials Example}
\uncover<1->{
$$
\mathrm{d}u_7     = \frac{\partial f_7}{\partial u_6} \mathrm{d} u_6 +
                    \frac{\partial f_7}{\partial u_5} \mathrm{d} u_5 +
                    \frac{\partial f_7}{\partial u_4} \mathrm{d} u_4 +
                    \frac{\partial f_7}{\partial u_3} \mathrm{d} u_3 +
                    \frac{\partial f_7}{\partial u_2} \mathrm{d} u_2 +
                    \frac{\partial f_7}{\partial u_1} \mathrm{d} u_1
$$
}
\uncover<2->{
Using the {\color{blue}{chain rule}}
$$
\frac{\partial u_7}{\partial u_6} =
                    \frac{\partial f_7}{\partial u_6} \frac{\partial u_6}{\partial u_6} +
                    \frac{\partial f_7}{\partial u_5} \frac{\partial u_5}{\partial u_6} +
                    \frac{\partial f_7}{\partial u_4} \frac{\partial u_4}{\partial u_6} +
                    \frac{\partial f_7}{\partial u_3} \frac{\partial u_3}{\partial u_6} +
                    \frac{\partial f_7}{\partial u_2} \frac{\partial u_2}{\partial u_6} +
                    \frac{\partial f_7}{\partial u_1} \frac{\partial
                      u_1}{\partial u_6}
$$
}
\uncover<3->{
Since $u_1, \dots, u_5$ do not depend on $u_6$
$$
\frac{\partial u_7}{\partial u_6} = \frac{\partial f_7}{\partial u_6}
= 1
$$
}
\end{frame}

\begin{frame}[fragile]{Differentials Example}
\uncover<1->{
Using the {\color{blue}{chain rule}} again
$$
\frac{\partial u_7}{\partial u_5} =
                    \frac{\partial f_7}{\partial u_6} \frac{\partial u_6}{\partial u_5} +
                    \frac{\partial f_7}{\partial u_5} \frac{\partial u_5}{\partial u_5} +
                    \frac{\partial f_7}{\partial u_4} \frac{\partial u_4}{\partial u_5} +
                    \frac{\partial f_7}{\partial u_3} \frac{\partial u_3}{\partial u_5} +
                    \frac{\partial f_7}{\partial u_2} \frac{\partial u_2}{\partial u_5} +
                    \frac{\partial f_7}{\partial u_1} \frac{\partial
                      u_1}{\partial u_5}
$$
}
\uncover<2->{
Since $u_1, \dots, u_4$ {\color{blue}{and}} $u_6$ do not depend on $u_5$
$$
\frac{\partial u_7}{\partial u_5} = \frac{\partial f_7}{\partial u_5}
= 1
$$
}
\end{frame}

\begin{frame}[fragile]{Differentials Example More Interesting}
Now things become more interesting as $u_6$ and $u_5$ both depend on
$u_4$.

$$
\begin{aligned}
\frac{\partial u_7}{\partial u_4} &=
 \frac{\partial u_7}{\partial u_6}\frac{\partial u_6}{\partial u_4} +
 \frac{\partial u_7}{\partial u_5}\frac{\partial u_5}{\partial u_4} \\
&= \frac{\partial u_7}{\partial u_6}\exp{u_4} +
   \frac{\partial u_7}{\partial u_5}\cos{u_5}
\end{aligned}
$$
\end{frame}

\begin{frame}[fragile]{Differentials Example Carrying On}
\uncover<1->{
$$
\begin{aligned}
 \frac{\partial u_7}{\partial u_3} =
 \frac{\partial u_7}{\partial u_4}\frac{\partial u_4}{\partial u_3} =
 \frac{\partial u_7}{\partial u_4}
\end{aligned}
$$
}
\uncover<2->{
$$
\begin{aligned}
\frac{\partial u_7}{\partial u_2} &=
 \frac{\partial u_7}{\partial u_4}\frac{\partial u_4}{\partial u_2} +
 \frac{\partial u_7}{\partial u_3}\frac{\partial u_3}{\partial u_2} \\
&= \frac{\partial u_7}{\partial u_4} + 2u_2\frac{\partial u_7}{\partial u_4} \\
\frac{\partial u_7}{\partial u_1} &=
 \frac{\partial u_7}{\partial u_2}\frac{\partial u_2}{\partial u_1} \\
&=\frac{\partial u_7}{\partial u_2}\exp{u_2}
\end{aligned}
$$
}
\end{frame}

\begin{frame}[fragile]{Reverse Mode Conclusions}
\begin{itemize}
\item
Note that having worked from top to bottom (the forward sweep) in the
graph to calculate the function itself, we have to work backwards from
bottom to top (the backward sweep) to calculate the derivative.
\item
So provided we can translate our program into a data flow graph, we can
apply this procedure to calculate the differential with the same
complexity as the original program.
\item
The pictorial representation of an ANN is effectively the data flow
graph of the cost function (without the final cost calculation itself)
and its differential can be calculated as just being identical to
backpropagation.
\end{itemize}
\end{frame}

\subsection{Forward Mode}

\begin{frame}[fragile]{Dual Numbers}
\begin{itemize}
\item
An alternative method for automatic differentiation is called forward
mode and has a simple implementation. Let us illustrate this using
Haskell. The actual implementation is about 20 lines of code.
\item
Let us define dual numbers

\begin{lstlisting}[language=Haskell]
data Dual = Dual Double Double
  deriving (Eq, Show)
\end{lstlisting}
\item
We can think of these pairs as first order polynomials in the
indeterminate $\epsilon$, $x + \epsilon x'$ such that $\epsilon^2 = 0$
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Dual Numbers}
Thus, for example, we have

$$
\begin{aligned}
(x + \epsilon x') + (y + \epsilon y') &= ((x + y) + \epsilon (x' + y')) \\
(x + \epsilon x')(y + \epsilon y') &= xy + \epsilon (xy' + x'y) \\
\log (x + \epsilon x') &=
\log x (1 + \epsilon \frac {x'}{x}) =
\log x + \epsilon\frac{x'}{x} \\
\sqrt{(x + \epsilon x')} &=
\sqrt{x(1 + \epsilon\frac{x'}{x})} =
\sqrt{x}(1 + \epsilon\frac{1}{2}\frac{x'}{x}) =
\sqrt{x} + \epsilon\frac{1}{2}\frac{x'}{\sqrt{x}} \\
\ldots &= \ldots
\end{aligned}
$$
\end{frame}

\begin{frame}[fragile]{Dual Numbers and The Chain Rule}
Notice that these equations implicitly encode the chain rule. For
example, we know, using the chain rule, that

$$
\frac{\mathrm{d}}{\mathrm{d} x}\log(\sqrt x) =
\frac{1}{\sqrt x}\frac{1}{2}x^{-1/2} =
\frac{1}{2x}
$$

And using the example equations above we have

$$
\begin{aligned}
\log(\sqrt {x + \epsilon x'}) &= \log (\sqrt{x} + \epsilon\frac{1}{2}\frac{x'}{\sqrt{x}}) \\
                              &= \log (\sqrt{x}) + \epsilon\frac{\frac{1}{2}\frac{x'}{\sqrt{x}}}{\sqrt{x}} \\
                              &= \log (\sqrt{x}) + \epsilon x'\frac{1}{2x}
\end{aligned}
$$
\end{frame}

\begin{frame}[fragile]{Dual Numbers Example}
Notice that dual numbers carry around the calculation and the
derivative of the calculation.

\begin{itemize}
\item
To actually evaluate $\log(\sqrt{x})$
at a particular value, say 2, we plug in 2 for $x$ and 1 for $x'$
$$
\log (\sqrt{2 + \epsilon 1}) = \log(\sqrt{2}) + \epsilon\frac{1}{4}
$$
\item
Thus the derivative of $\log(\sqrt{x})$ at 2 is $1/4$.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Dual Numbers are Numbers}
\begin{scriptsize}
\begin{lstlisting}[language=Haskell]
instance Num Dual where
  fromInteger n             = undefined
  (Dual x x') + (Dual y y') = Dual (x + y) (x' + y')
  (Dual x x') * (Dual y y') = Dual (x * y) (x * y' + y * x')
  negate (Dual x x')        = Dual (negate x) (negate x')
  signum _                  = undefined
  abs _                     = undefined
\end{lstlisting}
\end{scriptsize}
\end{frame}

\begin{frame}[fragile]{Dual Numbers Can Be Divided}
\begin{scriptsize}
\begin{lstlisting}[language=Haskell]
instance Fractional Dual where
  fromRational p    = undefined
  recip (Dual x x') = Dual (1.0 / x) (-x' / (x * x))
\end{lstlisting}
\end{scriptsize}
\end{frame}

\begin{frame}[fragile]{Dual Numbers are Quasi Floating Point}
\begin{scriptsize}
\begin{lstlisting}[language=Haskell]
instance Floating Dual where
  pi = constD pi
  exp   (Dual x x') = Dual (exp x)   (x' * exp x)
  log   (Dual x x') = Dual (log x)   (x' / x)
  sqrt  (Dual x x') = Dual (sqrt x)  (x' / (2 * sqrt x))
  sin   (Dual x x') = Dual (sin x)   (x' * cos x)
  cos   (Dual x x') = Dual (cos x)   (x' * (- sin x))
  sinh  (Dual x x') = Dual (sinh x)  (x' * cosh x)
  cosh  (Dual x x') = Dual (cosh x)  (x' * sinh x)
  asin  (Dual x x') = Dual (asin x)  (x' / sqrt (1 - x*x))
  acos  (Dual x x') = Dual (acos x)  (x' / (-sqrt (1 - x*x)))
  atan  (Dual x x') = Dual (atan x)  (x' / (1 + x*x))
  asinh (Dual x x') = Dual (asinh x) (x' / sqrt (1 + x*x))
  acosh (Dual x x') = Dual (acosh x) (x' / (sqrt (x*x - 1)))
  atanh (Dual x x') = Dual (atanh x) (x' / (1 - x*x))
\end{lstlisting}
\end{scriptsize}
\end{frame}


\begin{frame}[fragile]{Example}
That's all we need to do. Let us implement the function we considered
earlier.

\begin{scriptsize}
\begin{lstlisting}[language=Haskell]
f = h . h where h x = x * x
\end{lstlisting}
\end{scriptsize}

\begin{scriptsize}
\begin{lstlisting}
ghci> f (Dual 2 1)
  Dual 16.0 32.0
\end{lstlisting}
\end{scriptsize}
\end{frame}

\begin{frame}[fragile]{Automatic Differentiation is {\em Not} Symbolic
    Differentiation}
To see that we are *not* doing symbolic differentiation (it's easy to
see we are not doing numerical differentiation) let us step
through the actual evaluation.

$$
\begin{aligned}
f (\mathrm{Dual}\,2\,1) &\longrightarrow (\lambda z \rightarrow z
\times z \cdot \lambda z \rightarrow z \times z) (\mathrm{Dual}\,2\,1) \\
&\longrightarrow (\lambda z \rightarrow z
\times z) (\mathrm{Dual}\,(2 \times 2)\,(2 \times 1 + 2 \times 1)) \\
&\longrightarrow  (\lambda z \rightarrow z
\times z) (\mathrm{Dual}\,4\,4)\\
&\longrightarrow (\mathrm{Dual}\,(4 \times 4)\,(4 \times 4 + 4 \times 4)) \\
&\longrightarrow (\mathrm{Dual}\,16\,32)\\
\end{aligned}
$$
\end{frame}

\section{Application}

\begin{frame}[fragile]{Linear Regression}
$$
L(\boldsymbol{x}, \boldsymbol{y}, m, c) = \frac{1}{2n}\sum_{i=1}^n (y - (mx + c))^2
$$
\begin{scriptsize}
\begin{lstlisting}[language=Haskell]
cost m c xs ys = (sum (zipWith errSq xs ys)) /
                 (2 * (fromIntegral (length xs)))
  where
    errSq x y = z * z
      where
        z = y - (m * x + c)
\end{lstlisting}
\end{scriptsize}
\end{frame}

\begin{frame}[fragile]{Linear Regression}
\begin{scriptsize}
\begin{lstlisting}[language=Haskell]
g m c = cost m c xs ys

zs = (0.1, 0.1) : map f zs
  where

    deriv (Dual _ x') = x'

    f (c, m) = (c - gamma * cDeriv, m - gamma * mDeriv)
      where
        cDeriv = deriv (g (Dual m 0) (Dual c 1))
        mDeriv = deriv (flip g (Dual c 0) (Dual m 1))
\end{lstlisting}
\end{scriptsize}
\end{frame}

\begin{frame}[fragile]{Linear Regression}
\begin{scriptsize}
\begin{lstlisting}
ghci> take 10 zs
  [(0.1,0.1),
   (0.554,3.2240000000000006),
   (0.30255999999999983,1.4371599999999995),
   (0.4542824,2.4573704000000003),
   (0.3754896159999999,1.8730778559999997),
   (0.42839290304,2.2059302422400004),
   (0.4059525336255999,2.0145512305216),
   (0.42651316156582386,2.1228327781207037),
   (0.42242942391663607,2.059837404270339),
   (0.43236801802049607,2.0947533284323567)]
\end{lstlisting}
\end{scriptsize}
\end{frame}

\section{Concluding Thoughts}

\begin{frame}[fragile]{Efficiency}
Perhaps AD is underused because of efficiency?

It seems that the Financial Services industry is aware that AD is {\em
  more} efficient than current practice. Order of magnitude
improvements have been reported.
\begin{itemize}
\item
Smoking Adjoints: fast evaluation of Greeks in Monte Carlo Calculations
\item
Adjoints and automatic (algorithmic) differentiation in computational finance
\end{itemize}

Perhaps AD is slowly permeating into Machine Learning as well but
there seem to be no easy to find benchmarks.

\begin{itemize}
\item
Good Morning, Economics Blog
\item
Andrew Gelman's Blog
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Tools}
  If it were only possible to implement automatic differentiation in
  Haskell then its applicability would be somewhat
  limited. Fortunately this is not the case and it can be used in many
  languages. In general, there are three different approaches:

\begin{itemize}
\item Operator overloading: available for Haskell and C++. See the
  Haskell ad package and the C++ FADBAD approach using templates.
\item Source to source translators: available for Fortran, C and other
  languages e.g., ADIFOR, TAPENADE and see the wikipedia entry for a
  more comprehensive list.
\item New languages with built-in AD primitives.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Resources}
\begin{itemize}
\item \textcolor{blue}{http://idontgetoutmuch.wordpress.com}
\item
  \textcolor{blue}{http://en.wikipedia.org/wiki/Automatic\_differentiation}
\item \textcolor{blue}{http://www.autodiff.org}
\end{itemize}
\end{frame}

\end{document}
